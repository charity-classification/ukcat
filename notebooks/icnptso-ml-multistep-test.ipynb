{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify charities into ICNP/TSO categories\n",
    "\n",
    "A further test of the machine-learning model created in `icnptso-machine-learning-test.ipynb`, to see whether it's improved by turning it into a two-step model. First classifying charities into the top-level ICNPTSO groups, then classifying within each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "- `pandas` is used to manipulate the data\n",
    "- `sklearn.train_test_split` is used to split the sample data\n",
    "- `nltk` provides functions for preparing the data, plus a list of common stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\drkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create settings\n",
    "\n",
    "These settings hold the location of files used in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PICKLE_FILE = \"../data/icnptso_ml_model.pkl\"\n",
    "UKCAT_FILE = \"../data/ukcat.csv\"\n",
    "SAMPLE_FILE = \"../data/sample.csv\"\n",
    "TOP2000_FILE = \"../data/top2000.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the sample data\n",
    "\n",
    "Remove any records which don't have a ICNPTSO category included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(SAMPLE_FILE),\n",
    "        pd.read_csv(TOP2000_FILE),\n",
    "    ]\n",
    ").reset_index()\n",
    "df = df[df[\"ICNPTSO\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training data\n",
    "\n",
    "Create the text corpus by combining the name and activities data. `y` is the ICNPTSO code attached to the charity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6203"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.DataFrame([df[\"name\"], df[\"activities\"]]).T.apply(lambda x: \" \".join(x), axis=1)\n",
    "y = df[\"ICNPTSO\"].values\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare functions used to clean the text data before it's included in the machine learning models. \n",
    "\n",
    "[Lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) is the process where words are turned into the base for of the word - for example \"walking\" becomes \"walk\", \"better\" becomes \"good\".\n",
    "\n",
    "Stopwords (common words like \"and\", \"for\", \"of\") are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile(\"[/(){}\\[\\]\\|@,;]\")\n",
    "BAD_SYMBOLS_RE = re.compile(\"[^0-9a-z #+_]\")\n",
    "STOPWORDS = set(\n",
    "    stopwords.words(\"english\")\n",
    "    + [\n",
    "        \"trust\",\n",
    "        \"fund\",\n",
    "        \"charitable\",\n",
    "        \"charity\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "def lemma_words(doc):\n",
    "    return (lemma.lemmatize(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    text: a string\n",
    "\n",
    "    return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()  # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(\" \", text)  # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub(\"\", text)  # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = \" \".join(\n",
    "        lemma.lemmatize(word) for word in text.split() if word not in STOPWORDS\n",
    "    )  # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` is the list of cleaned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['college corpus christi blessed virgin mary university cambridge education undergraduate graduate student research work associated provision accommodation welfare catering service community scholar university cambridge'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = corpus.apply(clean_text).values\n",
    "np.random.choice(X, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce test and train datasets from `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a version of y with just the first letter of the code in (the group for the category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_group = np.array([cat[0] for cat in y_train])\n",
    "y_test_group = np.array([cat[0] for cat in y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer()),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", LogisticRegression(n_jobs=5, C=1e5, max_iter=1000)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=100000.0, max_iter=1000, n_jobs=5))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, y_train_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_group = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the accuracy of the group model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6913779210314263"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_group, y_test_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create individual models for the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_models = {\n",
    "    cat: Pipeline(\n",
    "        [\n",
    "            (\"vect\", CountVectorizer()),\n",
    "            (\"tfidf\", TfidfTransformer()),\n",
    "            (\"clf\", LogisticRegression(n_jobs=5, C=1e5, max_iter=1000)),\n",
    "        ]\n",
    "    )\n",
    "    for cat in sorted(set([cat[0] for cat in y]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through each group. For each group:\n",
    "\n",
    " - train the model on the training set\n",
    " - get the number of categories\n",
    " - test the model against the test set\n",
    " - compute the accuracy\n",
    " \n",
    "If there's only one sub-category then assign all the values to that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "Train: 702\n",
      "Training categories: A10, A11, A12, A19, A20, A21, A22, A29, A30, A90\n",
      "Test : 191\n",
      "Accuracy: 0.649\n",
      "\n",
      "B\n",
      "Train: 794\n",
      "Training categories: B10, B11, B12, B13, B19, B20, B21, B29, B30, B31, B32, B90\n",
      "Test : 195\n",
      "Accuracy: 0.590\n",
      "\n",
      "C\n",
      "Train: 379\n",
      "Training categories: C10, C11, C12, C13, C14, C19, C21, C22, C29, C31, C32, C39\n",
      "Test : 84\n",
      "Accuracy: 0.476\n",
      "\n",
      "D\n",
      "Train: 706\n",
      "Training categories: D10, D11, D12, D13, D14, D19, D20, D30, D31, D32, D33, D34, D41, D49, D90\n",
      "Test : 186\n",
      "Accuracy: 0.333\n",
      "\n",
      "E\n",
      "Train: 81\n",
      "Training categories: E10, E11, E12, E13, E14, E19, E20, E21, E22, E29, E90\n",
      "Test : 11\n",
      "Accuracy: 0.273\n",
      "\n",
      "F\n",
      "Train: 439\n",
      "Training categories: F12, F20, F30, F40\n",
      "Test : 111\n",
      "Accuracy: 0.613\n",
      "\n",
      "G\n",
      "Train: 465\n",
      "Training categories: G10, G11, G12, G13, G14, G15, G16, G19, G20, G22, G30\n",
      "Test : 114\n",
      "Accuracy: 0.360\n",
      "\n",
      "H\n",
      "Train: 534\n",
      "Training categories: H10, H90\n",
      "Test : 138\n",
      "Accuracy: 0.543\n",
      "\n",
      "I\n",
      "Train: 672\n",
      "Training categories: I10, I90\n",
      "Test : 176\n",
      "Accuracy: 0.648\n",
      "\n",
      "J\n",
      "Train: 62\n",
      "Training categories: J10, J20\n",
      "Test : 8\n",
      "Accuracy: 0.500\n",
      "\n",
      "K\n",
      "Train: 91\n",
      "Training categories: K10\n",
      "Test : 21\n",
      "Accuracy: 0.429\n",
      "\n",
      "L\n",
      "Train: 37\n",
      "Training categories: L30, L40, L50, L60\n",
      "Test : 6\n",
      "Accuracy: 1.000\n",
      "\n",
      "Overall accuracy: 0.533\n"
     ]
    }
   ],
   "source": [
    "multi_y_pred = []\n",
    "multi_y_test = []\n",
    "for cat, pipeline in group_models.items():\n",
    "    print(cat)\n",
    "    train_index = [c[0] == cat for c in y_train]\n",
    "    cat_X_train = X_train[train_index]\n",
    "    cat_y_train = y_train[train_index]\n",
    "    print(\"Train: {:,.0f}\".format(len(cat_X_train)))\n",
    "\n",
    "    training_categories = sorted(set(cat_y_train))\n",
    "    print(\"Training categories: {}\".format(\", \".join(training_categories)))\n",
    "\n",
    "    test_index = [c == cat for c in y_pred_group]\n",
    "    cat_X_test = X_test[test_index]\n",
    "    cat_y_test = y_test[test_index]\n",
    "    print(\"Test : {:,.0f}\".format(len(cat_X_test)))\n",
    "\n",
    "    # if there's only one category then just use that\n",
    "    if len(training_categories) == 1:\n",
    "        cat_y_pred = np.array([training_categories[0] for item in cat_y_test])\n",
    "    else:\n",
    "        pipeline.fit(cat_X_train, cat_y_train)\n",
    "        cat_y_pred = pipeline.predict(cat_X_test)\n",
    "\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy_score(cat_y_pred, cat_y_test)))\n",
    "\n",
    "    multi_y_pred.extend(cat_y_pred)\n",
    "    multi_y_test.extend(cat_y_test)\n",
    "    print()\n",
    "\n",
    "print(\"Overall accuracy: {:.3f}\".format(accuracy_score(multi_y_pred, multi_y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d88ce3146bd15929af18a4d80af41696be7c8ab5ccc06b4ae592a5dd9352cde0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
